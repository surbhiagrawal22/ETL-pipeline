{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing ETL Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.322076Z",
     "start_time": "2021-05-15T19:09:06.489099Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing all neccasary libraries for reading and manipulating data\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport      # for profiling the data\n",
    "import numpy as np\n",
    "import psycopg2                                 # for extracting/reading data from Postgres database\n",
    "from psycopg2 import Error\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import sys, os\n",
    "import credentials as creds                     # .py file storing all credential user id/password security purpose\n",
    "import logging\n",
    "from SQL_query import *                         # .py file which stores the SQL queries for connecting to MYSQL\n",
    "import pandas.io.sql as psql\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "from sqlalchemy import create_engine            # to load dataframe data into MYSQl tables and connect to MYSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logging file has been created to create a log of all errors/info while running the ETL, useful for debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.355759Z",
     "start_time": "2021-05-15T19:09:13.325049Z"
    }
   },
   "outputs": [],
   "source": [
    "# define connection database schema and table for reading dataset from Postgres connection\n",
    "schema='\"Accident_pipe\"'\n",
    "table='\"Accidents\"'\n",
    "\n",
    "# define the logging file \n",
    "logging_file=\"ETLlogfile.log\"\n",
    "logging.basicConfig(filename=logging_file, level=logging.INFO, filemode='w',\n",
    "                    format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "logger=logging.getLogger(__name__)\n",
    "\n",
    "# define the profile file\n",
    "profile_file=\"Oil_Pipeline_Accident.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for setting up connection to Postrgres server for extracting (reading) of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.392577Z",
     "start_time": "2021-05-15T19:09:13.360203Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data(schema, table):\n",
    " try: \n",
    "    connection_string = \"host=\"+ creds.PGHOST +\" port=\"+ \"5432\" +\\\n",
    "                        \" dbname=\"+ creds.PGDATABASE +\" user=\" +\\\n",
    "                        creds.PGUSER +\" password=\"+ creds.PGPASSWORD\n",
    "    \n",
    "    # Connect to database and Create a cursor object   \n",
    "    conn=psycopg2.connect(connection_string)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    logging.info(\"Connected Database Sucessfully! \\n\")\n",
    "    \n",
    "    # Reading the database schema and table     \n",
    "    sql_command3=f'select * from {schema}.{table}'\n",
    "    data = pd.read_sql(sql_command3, conn)\n",
    "    \n",
    "    logging.info('Step 1 -  Data loaded successfully \\n')\n",
    "    logging.info(' Dataframe head is - \\n {} \\n'.format(data.head()))\n",
    "    \n",
    " except Exception as e:   \n",
    "    logging.error(e)    \n",
    " finally:   \n",
    "    if (conn):\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        logging.info(\"PostgreSQL connection is closed \\n\")\n",
    "    return (data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining code for generating Profile Report for dataset in .html format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.416051Z",
     "start_time": "2021-05-15T19:09:13.405203Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_profile_report(data):\n",
    "    # Generating the profile report \n",
    "  try:  \n",
    "    prof = ProfileReport(data,minimal=True)\n",
    "    print(repr(prof.report))\n",
    "    print(prof.report.content)\n",
    "    prof.to_file(output_file=profile_file)\n",
    "    \n",
    "    rows,columns=data.shape[0],data.shape[1]\n",
    "    logging.info('Step 2 - Profile report generated and Initial data on load has {} rows and {} columns \\n'.format(rows,columns))\n",
    "    \n",
    "    print(f'Data has {data.shape[0]} rows and {data.shape[1]} columns \\n')     \n",
    "  except Exception as e:   \n",
    "    logging.error( e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping constant value columns as columns with constant values, will not help in making decisions or ML questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.442403Z",
     "start_time": "2021-05-15T19:09:13.424465Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_constant_value_columns(data):\n",
    "  try :  \n",
    "    drop_col=[e for e in data.columns if data[e].nunique() == 1]\n",
    "    data[drop_col].to_csv('Constant_value_column.csv',index=False,sep=';',header=True)\n",
    "    data.drop(drop_col,axis=1,inplace=True)\n",
    "    \n",
    "    logging.info(' Step 3 -  Constant values columns successfully  and stored in Constant_value_column.csv ') \n",
    "    logging.info(f'The constant columns dropped are {drop_col} \\n') \n",
    "    rows,columns=data.shape[0],data.shape[1]   \n",
    "    logging.info(' After removing constant value columns , data has now {} rows and {} columns \\n'.format(rows,columns))\n",
    "  except Exception as e:   \n",
    "    logging.error(e)\n",
    "  finally:\n",
    "    return data  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Uniqueness Dimension ,code to drop duplicate rows from dataset and also keeps a copy of dupliacted to seperate .csv file for ETL testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.489876Z",
     "start_time": "2021-05-15T19:09:13.466291Z"
    }
   },
   "outputs": [],
   "source": [
    "# to remove duplicate rows\n",
    "def removing_duplicated_rows(data):\n",
    "\n",
    " try:  \n",
    "     \n",
    "     duplicated_rows=data[data.duplicated(keep='first')]   \n",
    "    \n",
    "    # keep a copy of dupliacted rows in separate csv file for ETL testing purpose\n",
    "     duplicated_rows.to_csv('Duplicated_rows.csv', index=False,sep=';',header=True)\n",
    "        \n",
    "     data.drop_duplicates(inplace=True)   # Drop the rows where the whole row is duplicated   \n",
    "     data.drop_duplicates(inplace=True,subset=\"Report Number\",keep=\"first\") # to remove rows where the primary key Report number is duplicated \n",
    "        \n",
    "     \"\"\" removing any duplicated columns (If any column values are same) as Report Number and Report are\n",
    "     duplicate columns with same values ( as seen in profiling report )\"\"\" \n",
    "    \n",
    "     logging.info(' Step 4 - Duplicated rows deleted successfully and stored in Duplicated_rows.csv \\n')   \n",
    "     rows,columns=data.shape[0],data.shape[1]\n",
    "     logging.info('After removing duplicated rows, data has now {} rows and {} columns \\n'.format(rows,columns))\n",
    "    \n",
    " except Exception as e:   \n",
    "      logging.error(e)\n",
    " finally:\n",
    "      return data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniqueness Dimension , code to drops duplicated columns , Columns names may be different names but there values are same (In dataset, Report_Number and Report are duplicated columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.533027Z",
     "start_time": "2021-05-15T19:09:13.492099Z"
    }
   },
   "outputs": [],
   "source": [
    "  \n",
    "def removing_duplicated_columns(data):\n",
    "  try:\n",
    "    # Create an empty set\n",
    "    duplicateColumnNames = set()  \n",
    "    \n",
    "    # Iterate through all the columns of dataframe\n",
    "    for x in range(data.shape[1]): \n",
    "        \n",
    "        # Take column at xth index.\n",
    "        col = data.iloc[:, x]     \n",
    "        \n",
    "        # Iterate through all the columns in DataFrame from (x + 1)th index to last index\n",
    "        for y in range(x + 1, data.shape[1]): \n",
    "            \n",
    "            # Take column at yth index.\n",
    "            otherCol = data.iloc[:, y] \n",
    "            \n",
    "            # Check if two columns at x & y ,index are equal or not,if equal then adding to the set\n",
    "            if col.equals(otherCol):\n",
    "                duplicateColumnNames.add(data.columns.values[y])\n",
    "     \n",
    "    duplicateColumnNames=list(duplicateColumnNames)\n",
    "    # keep a copy of dupliacted columns in separate csv file for ETL testing purpose\n",
    "    data[duplicateColumnNames].to_csv(\"Duplicated_columns.csv\",index=False,sep=';',header=True)\n",
    "    # Dropping duplicate columns\n",
    "    data.drop(columns = duplicateColumnNames,axis=1,inplace=True)            \n",
    "   \n",
    "    logging.info(' Step 5 - Duplicated columns deleted successfully and file is stored in Duplicated_columns.csv \\n')   \n",
    "    logging.info('Names of duplicated columns deleted are \"{}\" \\n'.format(\" , \".join(duplicateColumnNames)   ) )  \n",
    "    rows,columns=data.shape[0],data.shape[1]\n",
    "    logging.info(' After removing duplicate columns , data has now {} rows and {} columns \\n'.format(rows,columns))   \n",
    "    \n",
    "  except error as e:\n",
    "    logging.info(e)\n",
    "  finally:    \n",
    "     return data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure consistency dimension and changing columns name ( space is replaced by \"_\" underscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.572559Z",
     "start_time": "2021-05-15T19:09:13.541976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Consistency Dimension\n",
    "def columns_name_change(data):\n",
    "  try:  \n",
    "    data.columns=data.columns.str.replace(\" \",\"_\")\n",
    "    \n",
    "    logging.info(' Step 6 - Column Name changed successfully \\n')\n",
    "    logging.info('New column names after name changed are {} \\n'.format(\" , \".join(data.columns.values)))\n",
    "  except Exception as e:   \n",
    "    logging.error(e)\n",
    "  finally:\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure consistency dimension and ensure all inconsistent values are replaced with consistent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.597356Z",
     "start_time": "2021-05-15T19:09:13.584908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Consistency Dimension\n",
    "def replace_inconsistent_values(data):\n",
    "  try : \n",
    "    \n",
    "    data['Liquid_Ignition']=data['Liquid_Ignition'].replace(to_replace=\"0\",value='NO')\n",
    "    data['Liquid_Ignition']=data['Liquid_Ignition'].replace(to_replace=\"1\",value='YES')\n",
    "    \n",
    "    data['Liquid_Explosion']=data['Liquid_Explosion'].replace(to_replace=\"1\",value='YES')\n",
    "    data['Liquid_Explosion']=data['Liquid_Explosion'].replace(to_replace=\"0\",value='NO')\n",
    "    \n",
    "    data['Pipeline_Type']=data['Pipeline_Type'].replace(to_replace=\"ABOVEGROUND TYPE\",value='ABOVEGROUND')\n",
    "    data['Pipeline_Type']=data['Pipeline_Type'].replace(to_replace=\"UNDERGROUND TYPE\",value='UNDERGROUND')\n",
    "    \n",
    "    data['Pipeline_Location']=data['Pipeline_Location'].replace(to_replace=\"ONSHORE LOCATION\",value='ONSHORE')\n",
    "    data['Pipeline_Location']=data['Pipeline_Location'].replace(to_replace=\"OFFSHORE LOCATION\",value='OFFSHORE')\n",
    "    \n",
    "    \n",
    "    logging.info(' Step 7 - Inconsistent values in columns changed successfully \\n')   \n",
    "    logging.info('Various values in  column Liquid_Ignition now are {} \\n'.format(\" , \".join(list(data['Liquid_Ignition'].unique())))) \n",
    "    logging.info('Various values in  column Liquid_Explosion now are {} \\n'.format(\" , \".join(list(data['Liquid_Explosion'].unique()))))\n",
    "    logging.info('Various values in  column Pipeline_Type now are {} \\n'.format(\" , \".join([ x for x in data.Pipeline_Type.unique() if x is not None ])))\n",
    "    logging.info('Various values in  column Pipeline_Location now are {} \\n'.format(\" , \".join(data.Pipeline_Location.unique())))\n",
    "  except Exception as e:   \n",
    "    logging.error(e)\n",
    "  finally:\n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Data types of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.610683Z",
     "start_time": "2021-05-15T19:09:13.602566Z"
    }
   },
   "outputs": [],
   "source": [
    "def changing_data_types(data):\n",
    "  try:  \n",
    "    data['Accident_Date/Time']= pd.to_datetime(data['Accident_Date/Time'].astype(str))\n",
    "    convert_type={\"Report_Number\":int,\"Supplemental_Number\":int,\"Pipeline_Shutdown\":bool,\"Operator_ID\":int}\n",
    "    data=data.astype(convert_type)\n",
    "    \n",
    "    logging.info(' Step 8 - Data types of columns changed successfully \\n')\n",
    "  except Exception as e:   \n",
    "    logging.error(e)\n",
    "  finally:\n",
    "    return data   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming white spaces before and after columns values to ensure consistency in values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.624821Z",
     "start_time": "2021-05-15T19:09:13.617273Z"
    }
   },
   "outputs": [],
   "source": [
    "# to remove white spaces before and after any value in a column\n",
    "def remove_white_spaces(data):\n",
    "  try:  \n",
    "    cols = data.select_dtypes(['object']).columns\n",
    "    data[cols] = data[cols].apply(lambda x: x.str.strip())\n",
    "    \n",
    "    logging.info(' Step 9- White space before and after column values changed successfully \\n')\n",
    "  except Exception as e:   \n",
    "    logging.error(e)\n",
    "  finally:\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to create star schema  ( 1 fact table , 7 Dimension table )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.647349Z",
     "start_time": "2021-05-15T19:09:13.631143Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_schema(data):\n",
    "    \n",
    "    # Accident Dimension Table\n",
    "    accident_details = data[['Accident_Year', 'Accident_Date/Time','Accident_County', 'Report_Number','Accident_City','Accident_State','Accident_Latitude', 'Accident_Longitude']]\n",
    "    \n",
    "    \n",
    "    # Cost dimension Table\n",
    "    cost_table = data[['Property_Damage_Costs','Lost_Commodity_Costs',\n",
    "                     'Public/Private_Property_Damage_Costs',\n",
    "                     'Emergency_Response_Costs', 'Environmental_Remediation_Costs',\n",
    "                     'Other_Costs', 'All_Costs', 'Report_Number']]\n",
    "    \n",
    "    # Fatalities dimension Table\n",
    "    fatalities_table = data[['Operator_Contractor_Fatalities', 'Other_Fatalities',\n",
    "                            'Public_Fatalities', 'All_Fatalities','Report_Number']]\n",
    "    \n",
    "    # Injuries Dimesion Table\n",
    "    injuries_table = data[['Operator_Employee_Injuries',\n",
    "                         'Operator_Contractor_Injuries', 'Public_Injuries', \n",
    "                         'All_Injuries','Report_Number']]\n",
    "    \n",
    "    # pipeline Dimension Table\n",
    "    pipeline_table = data[['Pipeline/Facility_Name',\n",
    "                         'Pipeline_Location', 'Pipeline_Type', 'Liquid_Type','Liquid_Ignition','Liquid_Explosion','Report_Number','Liquid_Subtype','Liquid_Name','Pipeline_Shutdown', 'Shutdown_Date/Time', 'Restart_Date/Time', 'Cause_Category','Cause_Subcategory']]\n",
    "    \n",
    "    \n",
    "    # Barrels Dimesion Table\n",
    "    barrels_table = data[['Unintentional_Release_(Barrels)',\n",
    "                       'Intentional_Release_(Barrels)', 'Liquid_Recovery_(Barrels)',\n",
    "                       'Net_Loss_(Barrels)','Report_Number']]\n",
    "    \n",
    "    #Operator Dimension Table\n",
    "    operator_table = data[['Operator_ID', 'Operator_Name']]\n",
    "    \n",
    "    # Facts Table\n",
    "    fact_table = data[['Report_Number','Supplemental_Number','Operator_ID','Public_Evacuations']]\n",
    "      \n",
    "    logging.info(' Step 10 - Snowflake schema created successfully ')\n",
    "    \n",
    "    return (accident_details,cost_table,fatalities_table,injuries_table,pipeline_table,barrels_table,operator_table,fact_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to save our star schema (dataframes) into MYSQL database using df.to_sql function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:13.665986Z",
     "start_time": "2021-05-15T19:09:13.653741Z"
    }
   },
   "outputs": [],
   "source": [
    "# converting pandas dataframe to SQL tables using sqlalchemy library module create_engine\n",
    "def dataframe_to_sql(accident_details,cost_table,fatalities_table,injuries_table,pipeline_table,barrels_table,operator_table,fact_table):  \n",
    "    engine = create_engine(f\"mysql://{user}:{password}@{host}/accident\")\n",
    "    with engine.begin() as connection:\n",
    "    \n",
    "      accident_details.to_sql(name=\"accident_detail\", con=connection, if_exists = 'replace', index=False)\n",
    "      cost_table.to_sql(name=\"accident_costs_table\", con=connection, if_exists = 'replace', index=False)\n",
    "      fatalities_table.to_sql(name=\"fatalities\", con=connection, if_exists = 'replace', index=False)\n",
    "      injuries_table.to_sql(name=\"injuries_table\", con=connection, if_exists = 'replace', index=False) \n",
    "      pipeline_table.to_sql(name=\"pipeline_table\", con=connection, if_exists = 'replace', index=False)\n",
    "      barrels_table.to_sql(name=\"barrel_table\", con=connection, if_exists = 'replace', index=False)\n",
    "      operator_table.to_sql(name=\"operator_table\", con=connection, if_exists = 'replace', index=False)\n",
    "      fact_table.to_sql(name=\"fact_table\", con=connection, if_exists = 'replace', index=False) \n",
    "      logging.info(\"Loading of staging files and star schema tables has been done in MySQL database succesfully \\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the main function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T19:09:43.135701Z",
     "start_time": "2021-05-15T19:09:13.669819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4154551abc284d2d8f86f61bf8758eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Summarize dataset'), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb652967877f4cd294e511396efeee85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Generate report structure'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Root\n",
      "{'body': Container(name=Root), 'footer': HTML, 'name': 'Root'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f2d4b05f29494b9a749a7832b3d232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Render HTML'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321bf09654ef4a089f4931f1924de6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Export report to file'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data has 2801 rows and 49 columns \n",
      "\n",
      "connection is successful <mysql.connector.connection.MySQLConnection object at 0x12737f2b0>\n",
      "Succesfully connected to Data Warehouse MYSQL\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "   data=load_data(schema,table)  \n",
    "\n",
    "   create_profile_report(data) \n",
    " \n",
    "   data=remove_constant_value_columns(data)\n",
    "   \n",
    "   data=removing_duplicated_rows(data)\n",
    "    \n",
    "   data=removing_duplicated_columns(data)\n",
    "\n",
    "   data=columns_name_change(data)     \n",
    "\n",
    "   data=replace_inconsistent_values(data)\n",
    "\n",
    "   data=changing_data_types(data)\n",
    "\n",
    "   data=remove_white_spaces(data)\n",
    "  \n",
    "   accident_details,cost_table,fatalities_table,injuries_table,pipeline_table,barrels_table,operator_table,fact_table=create_schema(data)\n",
    "    \n",
    "   connection=create_conenction(host,user,password)  # creating connection to MYSQL to save stageing files ( star flake schema file)\n",
    "   \n",
    "   cursor=create_cursor(connection) \n",
    "\n",
    "   create_db(cursor,connection)\n",
    "    \n",
    "   dataframe_to_sql(accident_details,cost_table,fatalities_table,injuries_table,pipeline_table,barrels_table,operator_table,fact_table )\n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
